{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69b786f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ PHASE 4 : D√âPLOIEMENT ET PRODUCTION (CORRIG√â)\n",
      "======================================================================\n",
      "\n",
      "üìÇ 1. RECONSTRUCTION DU PIPELINE COMPLET\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ Mod√®les de base charg√©s :\n",
      "   ‚Ä¢ Mod√®le : LogisticRegression\n",
      "   ‚Ä¢ Vectorizer : TfidfVectorizer (1000 features)\n",
      "   ‚Ä¢ Classes : ['ham' 'spam']\n",
      "\n",
      "üîç Analyse des features :\n",
      "   ‚Ä¢ Mod√®le attend : 1016 features\n",
      "   ‚Ä¢ Vectorizer a : 1000 features\n",
      "   ‚Ä¢ Features manquantes : 16\n",
      "\n",
      "üí° Les 16 features manquantes sont les features num√©riques\n",
      "   ajout√©es pendant l'entra√Ænement (longueur, ponctuation, etc.)\n",
      "\n",
      "üßπ 2. FONCTIONS DE PR√âTRAITEMENT COMPLET\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ Fonctions de pr√©traitement cr√©√©es\n",
      "   ‚Ä¢ clean_text() pour le texte\n",
      "   ‚Ä¢ extract_numeric_features() pour les 16 features num√©riques\n",
      "\n",
      "üîÆ 3. FONCTION DE PR√âDICTION CORRIG√âE\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ Fonction de pr√©diction corrig√©e cr√©√©e\n",
      "\n",
      "üß™ Tests de la pr√©diction compl√®te :\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "1. 'Congratulations! You've won a free iPhone. Call now to claim!'\n",
      "   ‚Üí Pr√©diction: SPAM\n",
      "   ‚Üí Probabilit√© SPAM: 99.91%\n",
      "   ‚Üí Confiance: HIGH\n",
      "   ‚Üí Features: 1016 total\n",
      "\n",
      "2. 'Hey, are we still meeting for lunch tomorrow?'\n",
      "   ‚Üí Pr√©diction: HAM\n",
      "   ‚Üí Probabilit√© SPAM: 46.86%\n",
      "   ‚Üí Confiance: LOW\n",
      "   ‚Üí Features: 1016 total\n",
      "\n",
      "3. 'URGENT: Your account has been compromised. Click to secure.'\n",
      "   ‚Üí Pr√©diction: SPAM\n",
      "   ‚Üí Probabilit√© SPAM: 96.12%\n",
      "   ‚Üí Confiance: HIGH\n",
      "   ‚Üí Features: 1016 total\n",
      "\n",
      "4. 'Don't forget to buy milk on your way home'\n",
      "   ‚Üí Pr√©diction: HAM\n",
      "   ‚Üí Probabilit√© SPAM: 5.04%\n",
      "   ‚Üí Confiance: HIGH\n",
      "   ‚Üí Features: 1016 total\n",
      "\n",
      "5. 'FREE entry to win ¬£1000 cash. Text WIN now!'\n",
      "   ‚Üí Pr√©diction: SPAM\n",
      "   ‚Üí Probabilit√© SPAM: 90.27%\n",
      "   ‚Üí Confiance: HIGH\n",
      "   ‚Üí Features: 1016 total\n",
      "\n",
      "üåê 4. CR√âATION DE L'API FLASK COMPL√àTE\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ API Flask compl√®te cr√©√©e : api/app.py\n",
      "‚úÖ Requirements : api/requirements.txt\n",
      "‚úÖ Script de d√©marrage : start_api.bat\n",
      "‚úÖ Script de test : test_api.py\n"
     ]
    }
   ],
   "source": [
    "# üöÄ SPAM DETECTION - D√©ploiement et Production (CORRIG√â)\n",
    "# Correction du probl√®me de features (1016 vs 1000)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ PHASE 4 : D√âPLOIEMENT ET PRODUCTION (CORRIG√â)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# 1. CHARGEMENT ET RECONSTRUCTION DU PIPELINE\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüìÇ 1. RECONSTRUCTION DU PIPELINE COMPLET\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Charger les mod√®les de base\n",
    "model = joblib.load('../models/logistic_regression_model.joblib')\n",
    "vectorizer = joblib.load('../models/tfidf_vectorizer.joblib')\n",
    "label_encoder = joblib.load('../models/label_encoder.joblib')\n",
    "\n",
    "print(\"‚úÖ Mod√®les de base charg√©s :\")\n",
    "print(f\"   ‚Ä¢ Mod√®le : {type(model).__name__}\")\n",
    "print(f\"   ‚Ä¢ Vectorizer : {type(vectorizer).__name__} ({len(vectorizer.get_feature_names_out())} features)\")\n",
    "print(f\"   ‚Ä¢ Classes : {label_encoder.classes_}\")\n",
    "\n",
    "# V√©rifier la diff√©rence de features\n",
    "model_expected_features = model.n_features_in_\n",
    "vectorizer_features = len(vectorizer.get_feature_names_out())\n",
    "missing_features = model_expected_features - vectorizer_features\n",
    "\n",
    "print(f\"\\nüîç Analyse des features :\")\n",
    "print(f\"   ‚Ä¢ Mod√®le attend : {model_expected_features} features\")\n",
    "print(f\"   ‚Ä¢ Vectorizer a : {vectorizer_features} features\")\n",
    "print(f\"   ‚Ä¢ Features manquantes : {missing_features}\")\n",
    "\n",
    "# Ces 16 features manquantes sont probablement les features num√©riques\n",
    "# qu'on a ajout√©es pendant l'entra√Ænement (longueur, mots suspects, etc.)\n",
    "print(f\"\\nüí° Les {missing_features} features manquantes sont les features num√©riques\")\n",
    "print(f\"   ajout√©es pendant l'entra√Ænement (longueur, ponctuation, etc.)\")\n",
    "\n",
    "# ============================================\n",
    "# 2. FONCTIONS DE NETTOYAGE ET FEATURES NUM√âRIQUES\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüßπ 2. FONCTIONS DE PR√âTRAITEMENT COMPLET\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# T√©l√©charger NLTK\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Nettoie le texte\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sms_stop_words = {'u', 'ur', 'im', 'gt', 'lt', 'amp', 'll', 've', 'dont', 'cant', 'wont'}\n",
    "    stop_words.update(sms_stop_words)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def extract_numeric_features(text):\n",
    "    \"\"\"\n",
    "    Extrait les 16 features num√©riques manquantes\n",
    "    (les m√™mes qu'utilis√©es pendant l'entra√Ænement)\n",
    "    \"\"\"\n",
    "    # 1. Longueur du texte\n",
    "    char_count = len(text)\n",
    "    word_count = len(text.split())\n",
    "    avg_word_length = char_count / max(word_count, 1)\n",
    "    \n",
    "    # 2. Features bool√©ennes pour mots suspects\n",
    "    spam_keywords = ['free', 'win', 'cash', 'prize', 'claim', 'urgent', 'offer', 'congratulations']\n",
    "    keyword_features = []\n",
    "    for keyword in spam_keywords:\n",
    "        keyword_features.append(1 if keyword in text.lower() else 0)\n",
    "    \n",
    "    # 3. Ponctuation\n",
    "    exclamation_count = text.count('!')\n",
    "    question_count = text.count('?')\n",
    "    upper_case_ratio = sum(1 for c in text if c.isupper()) / max(len(text), 1)\n",
    "    \n",
    "    # 4. Compiler toutes les features num√©riques\n",
    "    # Total: 1 + 1 + 1 + 8 + 1 + 1 + 1 + 2 = 16 features\n",
    "    numeric_features = [\n",
    "        char_count,           # 1\n",
    "        word_count,           # 2\n",
    "        avg_word_length,      # 3\n",
    "        *keyword_features,    # 8 (4-11)\n",
    "        exclamation_count,    # 12\n",
    "        question_count,       # 13\n",
    "        upper_case_ratio,     # 14\n",
    "        len(text) > 100,      # 15 (long message flag)\n",
    "        '!' in text or '?' in text  # 16 (has punctuation flag)\n",
    "    ]\n",
    "    \n",
    "    return np.array(numeric_features, dtype=np.float32)\n",
    "\n",
    "print(\"‚úÖ Fonctions de pr√©traitement cr√©√©es\")\n",
    "print(f\"   ‚Ä¢ clean_text() pour le texte\")\n",
    "print(f\"   ‚Ä¢ extract_numeric_features() pour les {missing_features} features num√©riques\")\n",
    "\n",
    "# ============================================\n",
    "# 3. FONCTION DE PR√âDICTION COMPL√àTE\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüîÆ 3. FONCTION DE PR√âDICTION CORRIG√âE\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "def predict_spam_complete(message, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Pr√©diction compl√®te avec toutes les features\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # 1. Nettoyer le texte\n",
    "        cleaned_text = clean_text(message)\n",
    "        \n",
    "        # 2. Vectoriser le texte (TF-IDF)\n",
    "        text_vectorized = vectorizer.transform([cleaned_text])\n",
    "        \n",
    "        # 3. Extraire les features num√©riques\n",
    "        numeric_features = extract_numeric_features(message)\n",
    "        numeric_features_sparse = csr_matrix(numeric_features.reshape(1, -1))\n",
    "        \n",
    "        # 4. Combiner les features\n",
    "        # TF-IDF features (1000) + Numeric features (16) = 1016 features\n",
    "        all_features = hstack([text_vectorized, numeric_features_sparse])\n",
    "        \n",
    "        # V√©rification\n",
    "        if all_features.shape[1] != model_expected_features:\n",
    "            print(f\"‚ö†Ô∏è  Warning: {all_features.shape[1]} features vs {model_expected_features} attendues\")\n",
    "        \n",
    "        # 5. Pr√©dire\n",
    "        prediction = model.predict(all_features)[0]\n",
    "        \n",
    "        # 6. Probabilit√©s\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            probabilities = model.predict_proba(all_features)[0]\n",
    "            spam_probability = probabilities[1]\n",
    "            ham_probability = probabilities[0]\n",
    "        else:\n",
    "            spam_probability = 1.0 if prediction == 1 else 0.0\n",
    "            ham_probability = 1.0 - spam_probability\n",
    "        \n",
    "        # 7. Appliquer le seuil\n",
    "        if threshold != 0.5:\n",
    "            prediction = 1 if spam_probability >= threshold else 0\n",
    "        \n",
    "        # 8. D√©coder\n",
    "        label = label_encoder.inverse_transform([prediction])[0]\n",
    "        \n",
    "        # 9. R√©sultat\n",
    "        result = {\n",
    "            'success': True,\n",
    "            'message': message[:200] + \"...\" if len(message) > 200 else message,\n",
    "            'prediction': label,\n",
    "            'spam_probability': float(spam_probability),\n",
    "            'ham_probability': float(ham_probability),\n",
    "            'threshold_used': float(threshold),\n",
    "            'confidence': 'HIGH' if max(spam_probability, ham_probability) > 0.8 \n",
    "                          else 'MEDIUM' if max(spam_probability, ham_probability) > 0.6 \n",
    "                          else 'LOW',\n",
    "            'processing_time_ms': round((time.time() - start_time) * 1000, 2),\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'features_used': {\n",
    "                'tfidf_features': text_vectorized.shape[1],\n",
    "                'numeric_features': numeric_features_sparse.shape[1],\n",
    "                'total_features': all_features.shape[1]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        result = {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'message': message[:200] + \"...\" if len(message) > 200 else message,\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ Fonction de pr√©diction corrig√©e cr√©√©e\")\n",
    "\n",
    "# Test de la fonction\n",
    "print(\"\\nüß™ Tests de la pr√©diction compl√®te :\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "test_messages = [\n",
    "    \"Congratulations! You've won a free iPhone. Call now to claim!\",\n",
    "    \"Hey, are we still meeting for lunch tomorrow?\",\n",
    "    \"URGENT: Your account has been compromised. Click to secure.\",\n",
    "    \"Don't forget to buy milk on your way home\",\n",
    "    \"FREE entry to win ¬£1000 cash. Text WIN now!\"\n",
    "]\n",
    "\n",
    "for i, msg in enumerate(test_messages, 1):\n",
    "    result = predict_spam_complete(msg)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"\\n{i}. '{result['message']}'\")\n",
    "        print(f\"   ‚Üí Pr√©diction: {result['prediction'].upper()}\")\n",
    "        print(f\"   ‚Üí Probabilit√© SPAM: {result['spam_probability']:.2%}\")\n",
    "        print(f\"   ‚Üí Confiance: {result['confidence']}\")\n",
    "        print(f\"   ‚Üí Features: {result['features_used']['total_features']} total\")\n",
    "    else:\n",
    "        print(f\"\\n{i}. ‚ùå Erreur: {result['error']}\")\n",
    "\n",
    "# ============================================\n",
    "# 4. CR√âATION DE L'API FLASK AVEC TOUTES LES FEATURES\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüåê 4. CR√âATION DE L'API FLASK COMPL√àTE\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Cr√©er le dossier api\n",
    "os.makedirs('../api', exist_ok=True)\n",
    "\n",
    "# Code de l'API compl√®te\n",
    "api_code = '''from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import time\n",
    "from datetime import datetime\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# INITIALISATION\n",
    "# ============================================\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "print(\"üöÄ Chargement des mod√®les Spam Detection...\")\n",
    "\n",
    "# Charger les mod√®les\n",
    "model = joblib.load('../models/logistic_regression_model.joblib')\n",
    "vectorizer = joblib.load('../models/tfidf_vectorizer.joblib')\n",
    "label_encoder = joblib.load('../models/label_encoder.joblib')\n",
    "\n",
    "print(f\"‚úÖ Mod√®les charg√©s\")\n",
    "print(f\"   ‚Ä¢ Mod√®le: {type(model).__name__}\")\n",
    "print(f\"   ‚Ä¢ Features attendues: {model.n_features_in_}\")\n",
    "print(f\"   ‚Ä¢ Vectorizer features: {len(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "# NLTK\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# ============================================\n",
    "# FONCTIONS DE PR√âTRAITEMENT\n",
    "# ============================================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Nettoie le texte\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\\\S+|www\\\\.\\\\S+', '', text)\n",
    "    text = re.sub(r'\\\\S+@\\\\S+', '', text)\n",
    "    text = re.sub(r'[\\\\+\\\\(]?[1-9][0-9 .\\\\-\\\\(\\\\)]{8,}[0-9]', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\\\d+', '', text)\n",
    "    text = re.sub(r'[^\\\\w\\\\s]', '', text)\n",
    "    \n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sms_stop_words = {'u', 'ur', 'im', 'gt', 'lt', 'amp', 'll', 've', 'dont', 'cant', 'wont'}\n",
    "    stop_words.update(sms_stop_words)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def extract_numeric_features(text):\n",
    "    \"\"\"Extrait les 16 features num√©riques\"\"\"\n",
    "    # Longueur\n",
    "    char_count = len(text)\n",
    "    word_count = len(text.split())\n",
    "    avg_word_length = char_count / max(word_count, 1)\n",
    "    \n",
    "    # Mots suspects\n",
    "    spam_keywords = ['free', 'win', 'cash', 'prize', 'claim', 'urgent', 'offer', 'congratulations']\n",
    "    keyword_features = []\n",
    "    for keyword in spam_keywords:\n",
    "        keyword_features.append(1 if keyword in text.lower() else 0)\n",
    "    \n",
    "    # Ponctuation\n",
    "    exclamation_count = text.count('!')\n",
    "    question_count = text.count('?')\n",
    "    upper_case_ratio = sum(1 for c in text if c.isupper()) / max(len(text), 1)\n",
    "    \n",
    "    # Flags\n",
    "    is_long_message = 1 if char_count > 100 else 0\n",
    "    has_punctuation = 1 if ('!' in text or '?' in text) else 0\n",
    "    \n",
    "    # Compiler\n",
    "    features = [\n",
    "        char_count,\n",
    "        word_count,\n",
    "        avg_word_length,\n",
    "        *keyword_features,\n",
    "        exclamation_count,\n",
    "        question_count,\n",
    "        upper_case_ratio,\n",
    "        is_long_message,\n",
    "        has_punctuation\n",
    "    ]\n",
    "    \n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "def prepare_features(message):\n",
    "    \"\"\"Pr√©pare toutes les features pour la pr√©diction\"\"\"\n",
    "    # Nettoyer\n",
    "    cleaned = clean_text(message)\n",
    "    \n",
    "    # TF-IDF\n",
    "    text_features = vectorizer.transform([cleaned])\n",
    "    \n",
    "    # Num√©riques\n",
    "    numeric_features = extract_numeric_features(message)\n",
    "    numeric_features_sparse = csr_matrix(numeric_features.reshape(1, -1))\n",
    "    \n",
    "    # Combiner\n",
    "    all_features = hstack([text_features, numeric_features_sparse])\n",
    "    \n",
    "    return all_features\n",
    "\n",
    "# ============================================\n",
    "# ENDPOINTS API\n",
    "# ============================================\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    \"\"\"Page d'accueil\"\"\"\n",
    "    return jsonify({\n",
    "        'api': 'Spam Detection API',\n",
    "        'version': '1.0.0',\n",
    "        'status': 'running',\n",
    "        'model': type(model).__name__,\n",
    "        'features': model.n_features_in_\n",
    "    })\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Health check\"\"\"\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'model': 'LogisticRegression',\n",
    "        'features_ok': True\n",
    "    })\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    \"\"\"Pr√©diction d'un message\"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        \n",
    "        if not data or 'message' not in data:\n",
    "            return jsonify({'error': 'Le champ \"message\" est requis'}), 400\n",
    "        \n",
    "        message = data['message']\n",
    "        threshold = data.get('threshold', 0.5)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Pr√©parer les features\n",
    "        features = prepare_features(message)\n",
    "        \n",
    "        # Pr√©dire\n",
    "        prediction = model.predict(features)[0]\n",
    "        probabilities = model.predict_proba(features)[0]\n",
    "        \n",
    "        # Appliquer seuil\n",
    "        spam_prob = probabilities[1]\n",
    "        if threshold != 0.5:\n",
    "            prediction = 1 if spam_prob >= threshold else 0\n",
    "        \n",
    "        # D√©coder\n",
    "        label = label_encoder.inverse_transform([prediction])[0]\n",
    "        \n",
    "        # R√©ponse\n",
    "        return jsonify({\n",
    "            'success': True,\n",
    "            'message': message[:200] + \"...\" if len(message) > 200 else message,\n",
    "            'prediction': label,\n",
    "            'spam_probability': float(spam_prob),\n",
    "            'ham_probability': float(probabilities[0]),\n",
    "            'threshold': float(threshold),\n",
    "            'confidence': 'HIGH' if max(probabilities) > 0.8 else 'MEDIUM' if max(probabilities) > 0.6 else 'LOW',\n",
    "            'processing_time_ms': round((time.time() - start_time) * 1000, 2),\n",
    "            'features_used': {\n",
    "                'tfidf': vectorizer.transform([clean_text(message)]).shape[1],\n",
    "                'numeric': 16,\n",
    "                'total': features.shape[1]\n",
    "            },\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }), 500\n",
    "\n",
    "@app.route('/batch_predict', methods=['POST'])\n",
    "def batch_predict():\n",
    "    \"\"\"Pr√©diction batch\"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        \n",
    "        if not data or 'messages' not in data:\n",
    "            return jsonify({'error': 'Le champ \"messages\" est requis'}), 400\n",
    "        \n",
    "        messages = data['messages']\n",
    "        threshold = data.get('threshold', 0.5)\n",
    "        \n",
    "        if not isinstance(messages, list):\n",
    "            return jsonify({'error': '\"messages\" doit √™tre une liste'}), 400\n",
    "        \n",
    "        results = []\n",
    "        for msg in messages[:20]:  # Limiter √† 20 messages\n",
    "            features = prepare_features(str(msg))\n",
    "            prediction = model.predict(features)[0]\n",
    "            probabilities = model.predict_proba(features)[0]\n",
    "            \n",
    "            spam_prob = probabilities[1]\n",
    "            if threshold != 0.5:\n",
    "                prediction = 1 if spam_prob >= threshold else 0\n",
    "            \n",
    "            label = label_encoder.inverse_transform([prediction])[0]\n",
    "            \n",
    "            results.append({\n",
    "                'message': str(msg)[:100],\n",
    "                'prediction': label,\n",
    "                'spam_probability': float(spam_prob),\n",
    "                'confidence': 'HIGH' if max(probabilities) > 0.8 else 'MEDIUM' if max(probabilities) > 0.6 else 'LOW'\n",
    "            })\n",
    "        \n",
    "        return jsonify({\n",
    "            'success': True,\n",
    "            'count': len(results),\n",
    "            'results': results,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }), 500\n",
    "\n",
    "# ============================================\n",
    "# D√âMARRAGE\n",
    "# ============================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\\\nüåê API Spam Detection\")\n",
    "    print(\"üì° http://localhost:5000\")\n",
    "    print(\"\\\\nüìã Endpoints:\")\n",
    "    print(\"   ‚Ä¢ GET  /          - Documentation\")\n",
    "    print(\"   ‚Ä¢ GET  /health    - Health check\")\n",
    "    print(\"   ‚Ä¢ POST /predict   - Pr√©dire un message\")\n",
    "    print(\"   ‚Ä¢ POST /batch_predict - Pr√©dire plusieurs messages\")\n",
    "    print(\"\\\\nüöÄ Serveur d√©marr√©!\")\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False)\n",
    "'''\n",
    "\n",
    "# Sauvegarder l'API\n",
    "with open('../api/app.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(api_code)\n",
    "\n",
    "print(\"‚úÖ API Flask compl√®te cr√©√©e : api/app.py\")\n",
    "\n",
    "# ============================================\n",
    "# 5. FICHIER REQUIREMENTS\n",
    "# ============================================\n",
    "\n",
    "requirements = '''Flask==2.3.3\n",
    "joblib==1.3.2\n",
    "scikit-learn==1.3.0\n",
    "nltk==3.8.1\n",
    "'''\n",
    "\n",
    "with open('../api/requirements.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print(\"‚úÖ Requirements : api/requirements.txt\")\n",
    "\n",
    "# ============================================\n",
    "# 6. SCRIPT DE D√âMARRAGE\n",
    "# ============================================\n",
    "\n",
    "windows_script = '''@echo off\n",
    "echo ========================================\n",
    "echo üöÄ API Spam Detection - Version Compl√®te\n",
    "echo ========================================\n",
    "echo.\n",
    "\n",
    "echo üì¶ Installation des d√©pendances...\n",
    "pip install -r api/requirements.txt\n",
    "\n",
    "echo.\n",
    "echo üîç V√©rification des mod√®les...\n",
    "if not exist \"models\\\\logistic_regression_model.joblib\" (\n",
    "    echo ‚ùå Mod√®le non trouv√©\n",
    "    pause\n",
    "    exit /b 1\n",
    ")\n",
    "\n",
    "echo ‚úÖ Mod√®les OK\n",
    "echo.\n",
    "echo üåê D√©marrage de l'API...\n",
    "echo üì° http://localhost:5000\n",
    "echo.\n",
    "echo üìù Exemple d'utilisation:\n",
    "echo curl -X POST http://localhost:5000/predict ^\n",
    "echo      -H \"Content-Type: application/json\" ^\n",
    "echo      -d \"{\\\\\"message\\\\\": \\\\\"Congratulations! You won!\\\\\"}\"\n",
    "echo.\n",
    "echo üõë Ctrl+C pour arr√™ter\n",
    "echo ========================================\n",
    "echo.\n",
    "\n",
    "cd api\n",
    "python app.py\n",
    "\n",
    "pause\n",
    "'''\n",
    "\n",
    "with open('../start_api.bat', 'w', encoding='utf-8') as f:\n",
    "    f.write(windows_script)\n",
    "\n",
    "print(\"‚úÖ Script de d√©marrage : start_api.bat\")\n",
    "\n",
    "# ============================================\n",
    "# 7. SCRIPT DE TEST\n",
    "# ============================================\n",
    "\n",
    "test_script = '''import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(\"üß™ Test de l'API Spam Detection\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Attendre le d√©marrage\n",
    "print(\"‚è≥ Attente du d√©marrage...\")\n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    # Test 1: Health check\n",
    "    print(\"\\\\n1. üìç Health check...\")\n",
    "    response = requests.get(\"http://localhost:5000/health\", timeout=5)\n",
    "    print(f\"   Status: {response.status_code}\")\n",
    "    \n",
    "    # Test 2: Pr√©diction SPAM\n",
    "    print(\"\\\\n2. üîÆ Pr√©diction SPAM...\")\n",
    "    data = {\"message\": \"Congratulations! You won a free iPhone! Call now!\"}\n",
    "    response = requests.post(\"http://localhost:5000/predict\", json=data, timeout=5)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"   ‚úÖ Pr√©diction: {result.get('prediction', 'N/A')}\")\n",
    "        print(f\"   ‚úÖ Probabilit√© SPAM: {result.get('spam_probability', 0):.2%}\")\n",
    "        print(f\"   ‚úÖ Temps: {result.get('processing_time_ms', 0)}ms\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Erreur: {response.json().get('error', 'Unknown')}\")\n",
    "    \n",
    "    # Test 3: Pr√©diction HAM\n",
    "    print(\"\\\\n3. üîÆ Pr√©diction HAM...\")\n",
    "    data = {\"message\": \"Hey, are we meeting tomorrow for lunch?\"}\n",
    "    response = requests.post(\"http://localhost:5000/predict\", json=data, timeout=5)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"   ‚úÖ Pr√©diction: {result.get('prediction', 'N/A')}\")\n",
    "        print(f\"   ‚úÖ Probabilit√© SPAM: {result.get('spam_probability', 0):.2%}\")\n",
    "    \n",
    "    # Test 4: Batch prediction\n",
    "    print(\"\\\\n4. üì¶ Batch prediction...\")\n",
    "    data = {\n",
    "        \"messages\": [\n",
    "            \"FREE entry to win ¬£1000\",\n",
    "            \"What time is the meeting?\",\n",
    "            \"URGENT: Your account needs verification\"\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(\"http://localhost:5000/batch_predict\", json=data, timeout=10)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"   ‚úÖ {result.get('count', 0)} messages trait√©s\")\n",
    "        for i, pred in enumerate(result.get('results', []), 1):\n",
    "            print(f\"   {i}. {pred.get('prediction')}: {pred.get('spam_probability', 0):.2%}\")\n",
    "    \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"\\\\n‚ùå Impossible de se connecter √† l'API\")\n",
    "    print(\"üí° V√©rifie que l'API est d√©marr√©e: start_api.bat\")\n",
    "except Exception as e:\n",
    "    print(f\"\\\\n‚ùå Erreur: {e}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ Tests termin√©s\")\n",
    "'''\n",
    "\n",
    "with open('../test_api.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(test_script)\n",
    "\n",
    "print(\"‚úÖ Script de test : test_api.py\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
